import pandas as pandas
import pgeocode as pgeocode
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
import numpy as np


class DataCleaner:
    def __init__(self, dataset_path):
        self.dataset_path = dataset_path
        self.full_df = pandas.read_csv(dataset_path)


        self.categorical_features = [
            'buildingCondition',
            'kitchenType',
            'subtype',
            'heatingType',
            'type'
        ]

        self.load_lat_lng_from_giraffe("data/Giraffe.csv",self.full_df)

    def preprocess_data(self,df) -> pandas.DataFrame:
        """
        Applies preprocessing steps (One-Hot Encoding with imputation)
        to specified categorical columns and passes others through.

        Returns:
        pd.DataFrame: The processed DataFrame with encoded and passthrough columns.
        """
        print("\n--- Starting data preprocessing ---")

        

        # --- Create a transformer pipeline for the categorical features to be encoded ---
        # Include imputation *within* this pipeline before encoding
        categorical_transformer_pipeline = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing_category')), # Handle NaNs with a specific placeholder
            ('onehot', OneHotEncoder(handle_unknown='ignore', drop='first', sparse_output=False)) # Then encode, drop first category
        ])

        # --- Create the ColumnTransformer ---
        # This will apply the categorical pipeline ONLY to the specified list of columns
        # and pass through all others.
        preprocessor = ColumnTransformer(
            transformers=[
                # Apply the categorical pipeline only to the columns in self.categorical_features
                ('cat_encoder', categorical_transformer_pipeline, self.categorical_features)
            ],
            # Use 'passthrough' for all other columns not explicitly listed in 'transformers'.
            # This handles numerical features, already-boolean features, etc.
            remainder='passthrough'
        )

        # Fit and transform the DataFrame using the ColumnTransformer
        # We are processing the entire DataFrame stored as self.df
        print("Fitting and transforming DataFrame using ColumnTransformer...")
        X_processed_array = preprocessor.fit_transform(df)

        print("Preprocessing complete.")
        print(f"Processed data shape (NumPy array): {X_processed_array.shape}")


        # --- Get the names of the new columns ---
        # The order in the output array will be:
        # 1. Columns from the first transformer ('cat_encoder')
        # 2. Columns from subsequent transformers (none in this case)
        # 3. Columns from 'remainder' ('passthrough')

        new_column_names = []

        # Get names from the 'cat_encoder' transformer
        if 'cat_encoder' in preprocessor.named_transformers_:
             # Access the OneHotEncoder step within the pipeline
             onehot_step = preprocessor.named_transformers_['cat_encoder'].named_steps['onehot']
             # Get the feature names generated by the OneHotEncoder
             new_column_names.extend(onehot_step.get_feature_names_out(input_features=self.categorical_features))

        # Get names for columns passed through by 'remainder'
        # These are the columns from the original self.df that were NOT in self.categorical_features
        all_input_cols = df.columns.tolist()
        processed_by_transformer_cols = self.categorical_features
        passthrough_cols = [col for col in all_input_cols if col not in processed_by_transformer_cols]
        new_column_names.extend(passthrough_cols)

        # Ensure the number of generated names matches the number of columns in the processed array
        if len(new_column_names) != X_processed_array.shape[1]:
            print(f"Warning: Mismatch between number of generated column names ({len(new_column_names)}) and processed array columns ({X_processed_array.shape[1]}). Column names might be incorrect.")
            # Fallback: Use generic names if mismatch occurs
            new_column_names = [f'feature_{i}' for i in range(X_processed_array.shape[1])]


        # Convert the processed NumPy array back to a pandas DataFrame
        # Ensure the original index is preserved
        X_processed_df = pandas.DataFrame(X_processed_array, columns=new_column_names, index=df.index)

        print("\nProcessed DataFrame created.")
        print(f"Processed DataFrame shape: {X_processed_df.shape}")
        print("Processed DataFrame Info:")
        X_processed_df.info()

        print("\n--- Data preprocessing finished ---")

        return X_processed_df
    


    def load_lat_lng_from_giraffe(self, giraffe_path,df):
        """Load latitude and longitude from Giraffe dataset."""

        df['id'] = pandas.to_numeric(df['id'], errors='coerce').astype('Int64')

        giraffe_df = pandas.read_csv(giraffe_path)
        giraffe_df = giraffe_df[['propertyId', 'latitude', 'longitude']]
        giraffe_df = giraffe_df.rename(columns={'propertyId': 'giraffe_id'})
        giraffe_df = giraffe_df.drop_duplicates(subset=['giraffe_id'], keep='first')
      
        df = df.merge(giraffe_df, left_on='id', right_on='giraffe_id', how='left')
        df = df.drop(columns=['giraffe_id'])

       

        #koala_df = pandas.read_csv("data/Koala.csv")
        #koala_df = koala_df[['property_ID', 'latitude', 'longitude']]
        #koala_df = koala_df.rename(columns={'property_ID': 'koala_id'})
        #koala_df = koala_df.drop_duplicates(subset=['koala_id'], keep='first')

        #self.df = self.df.merge(koala_df, left_on='id', right_on='koala_id', how='left')
        #self.df = self.df.drop(columns=['koala_id'])
        


        #self.df['latitude'] = self.df['latitude_x'].fillna(self.df['latitude_y'])
        #self.df['longitude'] = self.df['longitude_x'].fillna(self.df['longitude_y'])
        #self.df = self.df.drop(columns=['latitude_x', 'longitude_x', 'latitude_y', 'longitude_y'])
        #self.df.head(5)

        missing_lat_lng = df[df['latitude'].isna() | df['longitude'].isna()]
        if not missing_lat_lng.empty:
            #print("Missing latitude/longitude for the following IDs:")
            #print(missing_lat_lng['id'].tolist())
            df = df.dropna(subset="latitude") 
            df = df.dropna(subset="longitude")
            #print(f"DataCleaner::load_lat_lng_from_giraffe -> {self.df.shape}")

    def drop_useless_columns(self, columns_to_drop,df):
        """Drop specified columns from the dataset."""
        return df.drop(columns=columns_to_drop)

    def remove_duplicates(self,df):
        """Remove duplicate entries from the data."""
        df.duplicated(subset=["id"]).sum()
        dups = df.pivot_table(index = ['id'], aggfunc ='size')
        print(dups[dups > 1]) 
        df = df.drop_duplicates(subset=["id"], keep="first")

    def fill_missing_values_for_all(self,df):
         # bedroomCount : lets assume that they have at least one so fill nan by 1
        df['bedroomCount'] = df['bedroomCount'].fillna(1).astype(float)

        # bathroomCount same as bedrooms
        df['bathroomCount'] = df['bathroomCount'].fillna(1).astype(float)

        # toiletCount same as bedrooms
        df['toiletCount'] = df['toiletCount'].fillna(1).astype(float)

         # buildingCondition : replace by 'NOT_MENTIONED
        df['buildingCondition'] = df['buildingCondition'].fillna('NOT_MENTIONED')

         # hasThermicPanels lets assume that if its not precised, there are not
        df['hasThermicPanels'] = df['hasThermicPanels'].fillna(0).astype(float)

        # terraceOrientation
        #mode_terrace = df.loc[(df['hasTerrace'] == 1), 'terraceOrientation'].mode()[0]
        df.loc[(df['hasTerrace'] == 1) & (df['terraceOrientation'].isna()), 'terraceOrientation'] = -1
        df.loc[(df['hasTerrace'] != 1) & (df['terraceOrientation'].isna()), 'terraceOrientation'] = 'NO_TERRACE'

        #drop number of facade bigger than 4 and transform "facedeCount" into "facadeCount"
        df['facadeCount'] = df['facedeCount']
        df = df.drop(columns='facedeCount')
        df['facadeCount'] = df['facadeCount'].fillna(2).astype(int)
        df = df[df['facadeCount'] <= 4]

        province = {
            'Brussels': 5,

            'Antwerp': 11,
            'Flemish Brabant': 12,
            'East Flanders': 13,
            'West Flanders': 14,
            'Limburg': 15,

            'Luxembourg': 21,
            'LiÃ¨ge': 22,
            'Walloon Brabant': 23,
            'Namur': 24,
            'Hainaut': 25,
        }

        df['province'] = df["province"].replace(province)

        return df




    def fill_missing_values(self,df):
       
        # habitableSurface : replace by median 
        #df['habitableSurface'] = df['habitableSurface'].fillna(df['habitableSurface'].median())
        df['habitableSurface'] = df['habitableSurface'].fillna(-1) 
        

        # buildingConstructionYear
        #df['buildingConstructionYear'] = df['buildingConstructionYear'].fillna(df['buildingConstructionYear'].median()).astype(int)
        df['buildingConstructionYear'] = df['buildingConstructionYear'].fillna(-1).astype(int)
        

        # heatingType
        #df['heatingType'] = df['heatingType'].fillna(df['heatingType'].mode()[0])
        df['heatingType'] = df['heatingType'].fillna(-1)
        

        # kitchenType
        #df['kitchenType'] = df['kitchenType'].fillna(df['kitchenType'].mode()[0])
        df['kitchenType'] = df['kitchenType'].fillna(-1)
        

        # landSurface
        df['landSurface'] = df['landSurface'].fillna(-1)
        #df['landSurface'] = df['landSurface'].fillna(df['landSurface'].median())

        # livingRoomSurface
        df['livingRoomSurface'] = df['livingRoomSurface'].fillna(-1)
        #df['livingRoomSurface'] = df['livingRoomSurface'].fillna(df['livingRoomSurface'].median())

        # terraceSurface
        median_terrace = -1#df.loc[(df['hasTerrace'] == 1) & (df['terraceSurface'].notnull()), 'terraceSurface'].median()
        df.loc[(df['hasTerrace'] == 1) & (df['terraceSurface'].isna()), 'terraceSurface'] = median_terrace
        df.loc[(df['hasTerrace'] != 1) & (df['terraceSurface'].isna()), 'terraceSurface'] = 0
        
        
        # epcScore
        
        #df['epcScore'] = df['epcScore'].fillna(df['epcScore'].mode()[0])
        df['epcScore'] = df['epcScore'].fillna(-1)
        
        #df = df.dropna(subset=['terraceSurface','epcScore','hasTerrace','livingRoomSurface','landSurface','kitchenType','buildingConstructionYear','habitableSurface'])
        return df
    
    def convert_boolean_to_int(self,df):
        """Convert boolean values to integers."""
        binary_cols = [
        'hasBasement', 'hasLift', 'hasHeatPump', 'hasPhotovoltaicPanels', 
        'hasAirConditioning', 'hasArmoredDoor', 'hasVisiophone', 'hasOffice', 
        'hasSwimmingPool', 'hasFireplace', 'parkingCountIndoor', 'parkingCountOutdoor'
        ]

        for col in binary_cols:
            df[col] = df[col].map({True: 1, False: 0, 'True': 1, 'False': 0}).fillna(0).astype(int)

        # Colonnes dÃ©pendantes d'autres colonnes
        df['hasLivingRoom'] = df['hasLivingRoom'].map({True: 1, False: 0, 'True': 1, 'False': 0})
        df.loc[df['hasLivingRoom'].isna(), 'hasLivingRoom'] = df['livingRoomSurface'].notnull().astype(int)

        df['hasGarden'] = df['hasGarden'].map({True: 1, False: 0, 'True': 1, 'False': 0})
        df.loc[df['hasGarden'].isna(), 'hasGarden'] = df['gardenSurface'].notnull().astype(int)

        df['hasTerrace'] = df['hasTerrace'].map({True: 1, False: 0, 'True': 1, 'False': 0})
        df.loc[df['hasTerrace'].isna(), 'hasTerrace'] = df['terraceSurface'].notnull().astype(int)

        df['hasGarden'] = df['hasGarden'].map({True: 1, False: 0, 'True': 1, 'False': 0})

        # When hasLivingRoom = 0 ; livingRoomSurface = 0
        df.loc[df['hasLivingRoom'] == 0, 'livingRoomSurface'] = 0

        # When hasGarden = 0 ; gardenSurface = 0
        df.loc[df['hasGarden'] == 0, 'gardenSurface'] = 0

        # When hasTerrace = 0 ; terraceSurface = 0 and terraceOrientation = 0
        df.loc[df['hasTerrace'] == 0, 'terraceSurface'] = 0
        df.loc[df['hasTerrace'] == 0, 'terraceOrientation'] = 0

        #drop number of facade bigger than 4 and transform "facedeCount" into "facadeCount"
        df['facadeCount'] = df['facedeCount']
        df = df.drop(columns='facedeCount')
        df['facadeCount'] = df['facadeCount'].fillna(2)
        df = df[df['facadeCount'] <= 4]

        print(f"DataCleaner::convert_boolean_to_int -> {df.shape}")

    def transform_columns_type(self,df):
        """Transform the type of specified columns."""
        #'province': 'str', 'locality': 'str',
        col_types = {'id': 'int', 'type': 'str', 'subtype': 'str', 'bedroomCount': 'int', 'bathroomCount': 'int',
              'postCode': 'int', 'habitableSurface': 'float', 
             'hasBasement': 'int', 'buildingCondition': 'str',
             'buildingConstructionYear': 'int', 'hasLift': 'int', 'floodZoneType': 'str',
             'heatingType': 'str', 'hasHeatPump': 'int', 'hasPhotovoltaicPanels': 'int', 'hasThermicPanels': 'int',
             'kitchenType': 'str', 'landSurface': 'float', 'hasLivingRoom': 'int', 'livingRoomSurface': 'float',
             'hasGarden': 'int', 'gardenSurface': 'float', 'parkingCountIndoor': 'int', 'parkingCountOutdoor': 'int',
             'hasAirConditioning': 'int', 'hasArmoredDoor': 'int', 'hasVisiophone': 'int', 'hasOffice': 'int', 
             'toiletCount': 'int', 'hasSwimmingPool': 'int', 'hasFireplace': 'int', 'hasTerrace': 'int', 'terraceSurface': 'float',
             'terraceOrientation': 'str', 'epcScore': 'str', 'price': 'float', 'facadeCount': 'int'}

        for col, dtype in col_types.items():
            df[col] = df[col].astype(dtype)

    def transform_label_data(self,df):
        """Transform the label data to match the main dataframe."""
        #self.df["epcScore"] = self.df["epcScore"].map({
        #    'A++': 0, 'A+': 1, 'A': 2, 'B': 3, 'C': 4, 'D': 5, 'E': 6, 'F': 7, 'G': 8
        #}).astype(int)
        epc_order = ['A++', 'A+', 'A', 'B', 'C', 'D', 'E', 'F', 'G']
        df = df[df['epcScore'].isin(epc_order)]
        df["epcScore"] = df["epcScore"].map({
            'A++': 8, 'A+': 7, 'A': 6, 'B': 5, 'C': 4, 'D': 3, 'E': 2, 'F': 1, 'G': 0
        })
        df["epcScore"] = df["epcScore"].fillna(-1).astype(int)
        
        # floodZoneType lts assume that missing values are NON_FLOOD_ZONE
        df['floodZoneType'] = df['floodZoneType'].fillna('NON_FLOOD_ZONE')

        df["floodZoneType"] = df["floodZoneType"].map({
            'NON_FLOOD_ZONE': 0, 'CIRCUMSCRIBED_FLOOD_ZONE': 1, 'CIRCUMSCRIBED_WATERSIDE_ZONE': 1, 'POSSIBLE_FLOOD_ZONE': 1, 'POSSIBLE_N_CIRCUMSCRIBED_FLOOD_ZONE': 1,
            'POSSIBLE_N_CIRCUMSCRIBED_WATERSIDE_ZONE': 1, 'RECOGNIZED_FLOOD_ZONE': 1, 'RECOGNIZED_N_CIRCUMSCRIBED_FLOOD_ZONE': 1, 'RECOGNIZED_N_CIRCUMSCRIBED_WATERSIDE_FLOOD_ZONE': 1}).astype(int)
       
        building_condition_text = ['AS_NEW', 'JUST_RENOVATED','GOOD', 'TO_BE_DONE_UP', 'TO_RESTORE','TO_RENOVATE','NOT_MENTIONED']
        df = df[df['buildingCondition'].isin(building_condition_text)]
        df["buildingCondition"] = df["buildingCondition"].map({
            'AS_NEW': 6, 'JUST_RENOVATED': 5, 'GOOD': 4, 'TO_BE_DONE_UP': 3, 'TO_RESTORE': 2, 'TO_RENOVATE': 1,'NOT_MENTIONED' : -1
        })
        df["buildingCondition"] = df["buildingCondition"].fillna(-1).astype(int)

        kitchen_type_text = ['HYPER_EQUIPPED','USA_HYPER_EQUIPPED','INSTALLED','USA_INSTALLED', 'SEMI_EQUIPPED','USA_SEMI_EQUIPPED','NOT_INSTALLED', 'USA_UNINSTALLED']
        df = df[df['kitchenType'].isin(kitchen_type_text)]
        df["kitchenType"] = df["kitchenType"].map({
            'HYPER_EQUIPPED': 3, 'USA_HYPER_EQUIPPED': 4, 'INSTALLED': 2, 'USA_INSTALLED': 3, 'SEMI_EQUIPPED': 1, 'USA_SEMI_EQUIPPED': 2, 'NOT_INSTALLED': 0, 'USA_UNINSTALLED': 0
        })
        #df["kitchenType"] = df["kitchenType"].fillna(-1).astype(int)
        df['kitchenType'] = df['kitchenType'].fillna(-1).astype(int)


        str_cols = [
            'locality',
            'terraceOrientation', 
            #'epcScore', 
            'gardenOrientation', 
            #'kitchenType',
            'heatingType',
            #'floodZoneType',
            #'buildingCondition',
            'type',
            'subtype',
            ]



        for col in str_cols:
                df[col] = self.factorize_col(df, col)

        #df["buildingCondition"].dropna()
        return df
    
    def factorize_col(self,df, col):
        numbers, _ = pandas.factorize(df[col])
        return numbers + 1
    
    def get_lat_lng_from_postcode(self,df):
        nomi = pgeocode.Nominatim('BE')
        df["zipcode_Latitude"] = (nomi.query_postal_code(list(map(str,df["postCode"].tolist()))).latitude)
        df["zipcode_Longitude"] = (nomi.query_postal_code(list(map(str,df["postCode"].tolist()))).longitude)

    def drop_missing_price(self,df):
        """Drop rows with missing price."""
        return df.dropna(subset="price") 

    def dummies(self,df):
        return pandas.get_dummies(df,columns=['subtype','heatingType','type'],drop_first=True)


    def filter(self,df):
        df = df[df['price'] <= 1000000]
        #self.df = self.df[(self.df['type'] == "HOUSE") | (self.df['type'] == "APARTMENT")]

    def clean_data(self,columns_to_drop:list,debug=False) -> pandas.DataFrame:
        """Perform all cleaning operations."""

        pandas.set_option('display.max_columns', None)
        pandas.set_option('display.width', None)

        if debug:
            print("Debug mode is on.")
            print("DataCleaner::clean_data - >Percentage of nan values : ")
            print(f"DataCleaner::clean_data -> \n{(self.full_df.isna().mean() * 100).round(2).astype(str)} %")
            print(f"DataCleaner::clean_data -> Shape before cleaning : {self.full_df.shape}")
            print("-" * 50)

        
        self.get_lat_lng_from_postcode(self.full_df)
        self.full_df = self.drop_useless_columns(columns_to_drop,self.full_df)
        self.filter(self.full_df)
        self.convert_boolean_to_int(self.full_df)
        self.remove_duplicates(self.full_df)
        self.full_df = self.drop_missing_price(self.full_df)
        
        self.full_df = self.transform_label_data(self.full_df)
        self.full_df = self.fill_missing_values_for_all(self.full_df)

        with pandas.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):
            print((self.full_df.isna().mean() * 100).round(2).astype(str) + ' %')
        

        train_features, test_features = train_test_split(self.full_df, test_size = 0.25, random_state = 42)

        # Saving feature names for later use
        features_list = list(self.full_df.columns)

        train_df = pandas.DataFrame(train_features, columns=features_list)
        test_df = pandas.DataFrame(test_features, columns=features_list)
        
        train_df = self.fill_missing_values(train_df)
        test_df = self.fill_missing_values(test_df)
        
        self.transform_columns_type(train_df)
        self.transform_columns_type(test_df)

        #train_df = self.dummies(train_df)
        #test_df = self.dummies(test_df)

        train_df = self.transform_to_int(train_df)
        test_df = self.transform_to_int(test_df)



        train_df.merge(test_df)
        prices = train_df['price']
        train_df = train_df.drop(columns=['price'], axis=1)
        print("-"*100)
        with pandas.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):
            print((train_df.isna().mean() * 100).round(2).astype(str) + ' %')
       

        print("-"*100)

        # Convert to numpy array
        features = np.array(train_df)  # Uncommented this line to convert features to numpy array


        train_features, test_features, train_labels, test_labels = train_test_split(features,prices, test_size = 0.25, random_state = 42)

        
       
        return train_features, test_features, train_labels, test_labels, features_list
        
    
    def clean_test_data(self,columns_to_drop:list,debug=False) -> pandas.DataFrame:
        """Perform all cleaning operations."""

        pandas.set_option('display.max_columns', None)
        pandas.set_option('display.width', None)

        if debug:
            print("Debug mode is on.")
            print("DataCleaner::clean_data - >Percentage of nan values : ")
            print(f"DataCleaner::clean_data -> \n{(self.full_df.isna().mean() * 100).round(2).astype(str)} %")
            print(f"DataCleaner::clean_data -> Shape before cleaning : {self.full_df.shape}")
            print("-" * 50)

        
        self.get_lat_lng_from_postcode(self.full_df)
        self.full_df = self.drop_useless_columns(columns_to_drop,self.full_df)
        self.filter(self.full_df)
        self.convert_boolean_to_int(self.full_df)
        self.remove_duplicates(self.full_df)
        self.full_df = self.drop_missing_price(self.full_df)
        
        self.full_df = self.transform_label_data(self.full_df)
        self.full_df = self.fill_missing_values_for_all(self.full_df)

        with pandas.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):
            print((self.full_df.isna().mean() * 100).round(2).astype(str) + ' %')
        

       
        
        self.full_df = self.fill_missing_values(self.full_df)
        
        
        self.transform_columns_type(self.full_df)
        
        
        #self.full_df = self.dummies(self.full_df)
        self.full_df = self.transform_to_int(self.full_df)

        # Saving feature names for later use
        features_list = list(self.full_df.columns)

        
        prices = self.full_df['price']
        self.full_df = self.full_df.drop(columns=['price'], axis=1)
        print("-"*100)
        with pandas.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', None):
            print((self.full_df.isna().mean() * 100).round(2).astype(str) + ' %')
       

        print("-"*100)

        # Convert to numpy array
        features = np.array(self.full_df)  # Uncommented this line to convert features to numpy array


        train_features, test_features, train_labels, test_labels = train_test_split(features,prices, test_size = 0.25, random_state = 42)

        
       
        return train_features, test_features, train_labels, test_labels, features_list
    

    def transform_to_int(self,df):
        obj_cols = df.select_dtypes(include='object').columns
        df[obj_cols] = df[obj_cols].astype(int)
        return df
    

